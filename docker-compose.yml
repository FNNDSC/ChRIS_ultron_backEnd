# Docker Compose file for ChRIS backend development.
# See justfile for usage. *DO NOT* use the `docker compose` command directly.

services:
  chrisomatic:
    image: ghcr.io/fnndsc/chrisomatic:1.0.0
    profiles:
      - tools
    volumes:
      - "./chrisomatic:/etc/chrisomatic:ro"
      - "${DOCKER_SOCK:-/var/run/docker.sock}:/var/run/docker.sock"
    working_dir: /etc/chrisomatic
    userns_mode: host
    depends_on:
      - chris
    networks:
      - local

  chris:
    image: ${CUBE_IMAGE:-localhost/fnndsc/cube:dev}
    build:
      context: .
      args:
        ENVIRONMENT: local
    ports:
      - "8000:8000"
    volumes: &CHRIS_VOLUMES
      - chris_files:/data
      - ./chris_backend:/opt/app-root/src
    environment: &CHRIS_ENV
      DJANGO_SETTINGS_MODULE: "config.settings.local"
      STORAGE_ENV: "fslink"
      SPECTACULAR_SPLIT_REQUEST: "${SPECTACULAR_SPLIT_REQUEST-false}"
    stdin_open: true # docker run -i
    tty: true        # docker run -t
    command: python manage.py runserver 0.0.0.0:8000
    user: ${UID:?Please run me using just.}:${GID:?Please run me using just.}
    profiles:
      - cube
    depends_on: &CHRIS_DEPENDS
      db:
        condition: service_healthy
      rabbitmq:
        condition: service_started
      nats:
        condition: service_started
      cube-nonroot-user-volume-fix:
        condition: service_completed_successfully
    networks:
      local:
        aliases:
          - chrisdev.local  # hard-coded in chrisomatic/*.yml
    healthcheck:
      test: [ "CMD-SHELL", "echo curl -f http://localhost:8000/api/v1/users/ || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  worker-mains:
    image: ${CUBE_IMAGE:-localhost/fnndsc/cube:dev}
    command: celery -A core worker -c 4 -l info -Q main1,main2
    volumes: *CHRIS_VOLUMES
    environment: *CHRIS_ENV
    user: ${UID}:${GID}
    profiles:
      - cube
    depends_on: *CHRIS_DEPENDS
    networks:
      - local

  worker-periodic:
    image: ${CUBE_IMAGE:-localhost/fnndsc/cube:dev}
    command: celery -A core worker -c 2 -l info -Q periodic
    volumes: *CHRIS_VOLUMES
    environment: *CHRIS_ENV
    user: ${UID}:${GID}
    profiles:
      - cube
    depends_on: *CHRIS_DEPENDS
    networks:
      - local

  celery-scheduler:
    image: ${CUBE_IMAGE:-localhost/fnndsc/cube:dev}
    command: celery -A core beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    volumes: *CHRIS_VOLUMES
    environment: *CHRIS_ENV
    user: ${UID}:${GID}
    profiles:
      - cube
    depends_on: *CHRIS_DEPENDS
    networks:
      - local

  db:
    image: docker.io/library/postgres:18
    volumes:
      - db:/var/lib/postgresql
    environment:
      - POSTGRES_DB=chris_dev
      - POSTGRES_USER=chris
      - POSTGRES_PASSWORD=Chris1234
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 2s
      timeout: 4s
      retries: 3
      start_period: 60s
    networks:
      local:
        aliases:
          - chris_dev_db  # hard-coded in chris_backend/config/settings/local.py

  rabbitmq:
    image: docker.io/library/rabbitmq:4
    volumes:
      - rabbitmq:/var/lib/rabbitmq
    networks:
      local:
        aliases:
          - queue  # hard-coded in chris_backend/config/settings/local.py

  pfcon:
    image: ${PFCON_IMAGE:-ghcr.io/fnndsc/pfcon:latest}
    environment:
      SECRET_KEY: secret
      PFCON_USER: pfcon
      PFCON_PASSWORD: pfcon1234
      PFCON_INNETWORK: "true"
      STORAGE_ENV: "fslink"
      STOREBASE_MOUNT: /var/local/storeBase
      CONTAINER_ENV: docker
      CONTAINER_USER: "${UID}:${GID}"
      ENABLE_HOME_WORKAROUND: "yes"
      JOB_LABELS: "org.chrisproject.miniChRIS=plugininstance"
      REMOVE_JOBS: "yes"
    # port is hard-coded in chris_backend/config/settings/local.py
    command: gunicorn -b 0.0.0.0:30005 -w 8 -t 120 pfcon.wsgi:application
    ports:
      - "30005:30005"
    volumes:
      - chris_files:/var/local/storeBase
      - "${DOCKER_SOCK:-/var/run/docker.sock}:/var/run/docker.sock"
    networks:
      local:
        aliases:
          - pfcon.remote  # hard-coded in chris_backend/config/settings/local.py
    userns_mode: "host"
    labels:
      org.chrisproject.role: "pfcon"

  nats:
    image: docker.io/library/nats:2.10.20-alpine3.20
    ports:
      - "4222:4222"
      - "8222:8222"
    networks:
      local:

  lldap:
    image: docker.io/nitnelave/lldap:stable
    ports:
      - "3890:3890"
      - "17170:17170"
    volumes:
      - "lldap:/data"
    environment:
      TZ: America/New_York
      LLDAP_JWT_SECRET: super_secret_random_string
      LLDAP_LDAP_USER_PASS: chris1234
      LLDAP_LDAP_BASE_DN: dc=example,dc=org
    networks:
      local:

  cube-nonroot-user-volume-fix:
    image: docker.io/library/alpine:latest
    volumes:
      - chris_files:/data
    user: root
    command: chmod 777 /data
    restart: "no"

  openapi-generator:
    image: docker.io/openapitools/openapi-generator-cli:v7.8.0
    volumes:
      - ${OPENAPI_GENERATOR_OUTPUT:-./clients}:/out:rw
    user: ${UID:?Please run me using just.}:${GID:?Please run me using just.}
    profiles:
      - cube
      - tools
    networks:
      - local
    depends_on:
      chris:
        condition: service_healthy

  orthanc:
    image: docker.io/jodogne/orthanc-plugins:1.12.3
    volumes:
      - ./orthanc.json:/etc/orthanc/orthanc.json:ro
      - orthanc:/var/lib/orthanc/db
    ports:
      - "4242:4242"
      - "8042:8042"
    networks:
      - local

  pfdcm:
    image: ghcr.io/fnndsc/pfdcm:3.1.2
    container_name: pfdcm
    environment:
      MAX_WORKERS: 1
    volumes:
      - pfdcm:/home/dicom:rw
      - ./pfdcm-services:/home/dicom/services:ro
    ports:
      - "4005:4005"
    networks:
      - local
    user: "1001"
    depends_on:
      pfdcm-nonroot-user-volume-fix:
        condition: service_completed_successfully

  pfdcm-nonroot-user-volume-fix:
    image: docker.io/library/alpine:latest
    volumes:
      - pfdcm:/home/dicom:rw
    user: root
    command: chown 1001 /home/dicom
    restart: "no"

  oxidicom:
    image: ghcr.io/fnndsc/oxidicom:3.0.0
    environment:
      # https://chrisproject.org/docs/oxidicom/deployment#environment-variables
      OXIDICOM_FILES_ROOT: /data
      OXIDICOM_AMQP_ADDRESS: amqp://queue:5672
      OXIDICOM_NATS_ADDRESS: nats:4222
      OXIDICOM_SCP_AET: ChRIS
      OXIDICOM_SCP_PROMISCUOUS: "true"
      OXIDICOM_LISTENER_THREADS: 32
      OXIDICOM_LISTENER_PORT: 11111
      RUST_LOG: oxidicom=info
      OXIDICOM_PROGRESS_INTERVAL: 100ms
      OXIDICOM_DEV_SLEEP: 150ms  # throttle performance, so that we can see what's going on in ChRIS_ui
    ports:
      - "11111:11111"
    volumes:
      - chris_files:/data:rw
    networks:
      - local
    user: ${UID}:${GID}
    stop_signal: SIGKILL
    depends_on:
      rabbitmq:
        condition: service_started
      nats:
        condition: service_started
      cube-nonroot-user-volume-fix:
        condition: service_completed_successfully


volumes:
  chris_files:
  db:
  lldap:
  rabbitmq:
  orthanc:
  pfdcm:

networks:
  local:
