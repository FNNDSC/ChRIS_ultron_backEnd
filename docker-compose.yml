# https://docs.docker.com/compose/yml/
# Each service defined in docker-compose.yml must specify exactly one of
# image or build. Other keys are optional, and are analogous to their
# docker run command-line counterparts.
#
# As with docker run, options specified in the Dockerfile (e.g., CMD,
# EXPOSE, VOLUME, ENV) are respected by default - you don't need to
# specify them again in docker-compose.yml.
#

version: '3'

services:

  chris:
    image: ${CREPO}/chris
    env_file:
      - ./secrets/.chris.env
      - ./secrets/.chris_db.env
      - ./secrets/.swift_service.env
      - ./secrets/.pfcon_service.env
    ports:
      - "8000:8000"
    depends_on:
      - chris_db
      - swift_service
      - queue
      - chrisstore
      - pfcon_service
    restart: on-failure
    labels:
      name: "ChRIS_ultron_backEnd"
      role: "Production server using Apache's mod_wsgi"

  worker:
    image: ${CREPO}/chris
    entrypoint: ''
    command: celery -A core worker -c 1 -l info -Q main
    env_file:
      - ./secrets/.chris.env
      - ./secrets/.chris_db.env
      - ./secrets/.swift_service.env
      - ./secrets/.pfcon_service.env
    depends_on:
      - chris_db
      - swift_service
      - queue
      - pfcon_service
    restart: on-failure
    labels:
      name: "ChRIS_ultron_backEnd Asynchronous Worker"
      role: "Production async worker"

  chris_db:
    image: mysql:5
    volumes:
      - chris_db_data:/var/lib/mysql
    env_file:
      - ./secrets/.chris_db.env
    restart: on-failure
    labels:
      name: "ChRIS_ultron_backEnd MySQL Database"
      role: "Production MySQL database"

  queue:
    image: rabbitmq:3
    restart: on-failure
    labels:
      name: "ChRIS_ultron_backEnd Asynchronous Task Queue"
      role: "Production async task queue"

  chrisstore:
    image: ${CREPO}/chris_store
    env_file:
      - ./secrets/.chris_store.env
      - ./secrets/.chris_store_db.env
      - ./secrets/.swift_service.env
    ports:
      - "8010:8010"
    depends_on:
      - chris_store_db
      - swift_service
    restart: on-failure
    labels:
      name: "ChRIS_store"
      role: "Chris store service"

  chris_store_db:
    image: mysql:5
    volumes:
      - chris_store_db_data:/var/lib/mysql
    env_file:
      - ./secrets/.chris_store_db.env
    restart: on-failure
    labels:
      name: "ChRIS_store MySQL Database"
      role: "Chris store database"

  swift_service:
    image: fnndsc/docker-swift-onlyone
    volumes:
      - swift_storage:/srv
    env_file:
      - ./secrets/.swift_service.env
    ports:
      - "8080:8080"
    restart: on-failure
    labels:
      name: "swift"
      role: "swift object storage service"

  pfcon_service:
    image: ${CREPO}/pfcon
    volumes:
      - ./FS/data:/data
    command: ["--forever", "--httpResponse", "--verbosity", "1"]
    env_file:
      - ./secrets/.pfcon_service.env
      - ./secrets/.swift_service.env
    ports:
      - "5005:5005"
    depends_on:
      - swift_service
      - pman_service
      - pfioh_service
    restart: on-failure
    labels:
      name: "pfcon"
      role: "pfcon service"

  pfioh_service:
    # The following is a bit complicated... Basically we need to map a physical dir
    # in the HOST to the key store in pfioh. The keystore is specified by the
    # --storeBase flag.
    image: ${CREPO}/pfioh
    command: ["--forever", "--httpResponse", "--createDirsAsNeeded", "--storeBase", "/hostFS/storeBase", "--verbosity", "1"]
    volumes:
      - ./FS/remote:/hostFS/storeBase
    ports:
      - "5055:5055"
    restart: on-failure
    labels:
      name: "pfioh"
      role: "pfioh service"

  pman_service:
    # pman also needs access to the pfioh storeBase folder (and hence the volume)
    # mapping from the HOST file system space.
    #
    # There is however an extra twist. Since pman spins off containers of its
    # own, it needs to mount this storeBase dir into the spawned container.
    # However, it can't mount a dir inside itself to the spawned container,
    # it has to mount an actaul existing directory. This directory is
    # passed in the SHAREDIRBASE env variable, and will override the base
    # mapping of SHAREDIR inside pman to the swarm manager. Note that the
    # full path to the spawned container should be ${SHAREDIRBASE} + keyStore!
    environment:
      - STOREBASE
    image: ${CREPO}/pman
    command: ["--rawmode", "1", "--http", "--port", "5010", "--listeners", "12", "--verbosity", "1"]
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./FS/remote:/hostFS/storeBase
    ports:
      - "5010:5010"
    restart: on-failure
    labels:
      name: "pman"
      role: "pman service"

volumes:
  chris_db_data:
  chris_store_db_data:
  swift_storage:
